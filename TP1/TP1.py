# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NtsSSTTQkJI5V-F7we0E-2VamDZ9D1ii
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Configuration
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (14, 6)

print("="*80)
print("TP PARTIE 1 â€” STATISTIQUES ET LOI NORMALE EN FINANCE")
print("Analyse risque portefeuille et calcul VaR")
print("="*80)

# ============================================================================ # DONNÃ‰ES
# ============================================================================

# Rendements mensuels historiques (%)
rendements_A = np.array([
    1.2, 0.8, -0.5, 1.5, 0.9, 1.1, 0.7, 1.3, 1.0, 0.6, 1.4, 0.8,
    1.1, 0.9, -0.3, 1.2, 1.0, 1.5, 0.8, 1.3, 0.9, 1.1, 1.2, 1.0
])

rendements_B = np.array([
    4.5, -2.1, 6.2, -3.5, 5.8, 7.1, -1.8, 4.9, 3.2, -4.2, 8.5, -2.7,
    5.1, 6.8, -3.1, 7.3, 4.5, -2.9, 6.7, 5.3, -3.8, 7.9, 4.2, 5.5
])

# ParamÃ¨tres
capital = 500000 # â‚¬ Ã  investir
perte_max_toleree = 50000 # â‚¬ (10% capital)
taux_sans_risque = 3.0 # % annuel

# ============================================================================ # QUESTION 1.1 â€” STATISTIQUES DESCRIPTIVES
# ============================================================================

print("\n" + "="*80)
print("QUESTION 1.1 â€” STATISTIQUES DESCRIPTIVES")
print("="*80)

def calculer_stats_portefeuille(rendements, nom):
    """
    Calcule statistiques descriptives portefeuille

    Parameters:
    rendements : np.array Rendements mensuels (%)
    nom : str
    Nom portefeuille
    Returns:
    dict : Statistiques calculÃ©es
    """
    # a) Moyenne mensuelle
    moyenne_mensuelle = np.mean(rendements)

    # b) Ã‰cart-type mensuel
    ecart_type_mensuel = np.std(rendements, ddof=1) # ddof=1 pour Ã©chantillon

    # c) MÃ©diane
    mediane = np.median(rendements)

    # d) Rendement annualisÃ© (capitalisation composÃ©e)
    # Formule : (1 + r_mensuel/100)^12 - 1
    rendement_annuel = ((1 + moyenne_mensuelle/100)**12 - 1) * 100

    # e) VolatilitÃ© annualisÃ©e
    # Formule : Ïƒ_annuel = Ïƒ_mensuel Ã— âˆš12
    volatilite_annuelle = ecart_type_mensuel * np.sqrt(12)

    s_dict = {
        'nom': nom,
        'moyenne_mensuelle': moyenne_mensuelle,
        'ecart_type_mensuel': ecart_type_mensuel,
        'mediane': mediane,
        'rendement_annuel': rendement_annuel,
        'volatilite_annuelle': volatilite_annuelle
    }

    return s_dict

# Calcul stats pour les deux portefeuilles
stats_A = calculer_stats_portefeuille(rendements_A, "CONSERVATIVE (A)")
stats_B = calculer_stats_portefeuille(rendements_B, "AGRESSIF (B)")

# Affichage rÃ©sultats
for s in [stats_A, stats_B]: # Renamed loop variable from 'stats' to 's'
    print(f"\nðŸ“Š PORTEFEUILLE {s['nom']}")
    print(f"\tâ€¢ Rendement mensuel moyen : {s['moyenne_mensuelle']:.2f}%")
    print(f"\tâ€¢ Ã‰cart-type mensuel : {s['ecart_type_mensuel']:.2f}%")
    print(f"\tâ€¢ MÃ©diane : {s['mediane']:.2f}%")
    print(f"\tâ€¢ Rendement annualisÃ© : {s['rendement_annuel']:.2f}%")
    print(f"\tâ€¢ VolatilitÃ© annualisÃ©e : {s['volatilite_annuelle']:.2f}%")

# ============================================================================ # QUESTION 1.2 â€” VISUALISATION DISTRIBUTIONS
# ============================================================================

print("\n" + "="*80)
print("QUESTION 1.2 â€” VISUALISATION DISTRIBUTIONS")
print("="*80)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Subplot 1 : Histogrammes superposÃ©s
ax1 = axes[0]
ax1.hist(rendements_A, bins=10, alpha=0.6, color='green', edgecolor='black', label='Portefeuille A (Conservative)', density=True)
ax1.hist(rendements_B, bins=10, alpha=0.6, color='red', edgecolor='black', label='Portefeuille B (Agressif)', density=True)

# Lignes moyennes
ax1.axvline(stats_A['moyenne_mensuelle'], color='darkgreen', linestyle='--', linewidth=2, label=f'Moyenne A = {stats_A["moyenne_mensuelle"]:.2f}%')
ax1.axvline(stats_B['moyenne_mensuelle'], color='darkred', linestyle='--', linewidth=2, label=f'Moyenne B = {stats_B["moyenne_mensuelle"]:.2f}%')

ax1.set_title('Distributions rendements mensuels', fontsize=12, fontweight='bold')
ax1.set_xlabel('Rendement mensuel (%)')
ax1.set_ylabel('DensitÃ©')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# Subplot 2 : Boxplots comparatifs
ax2 = axes[1]
data_boxplot = [rendements_A, rendements_B]
bp = ax2.boxplot(data_boxplot, labels=['Portefeuille A', 'Portefeuille B'], patch_artist=True, widths=0.6)

# Couleurs boxplots
colors = ['lightgreen', 'lightcoral']
for patch, color in zip(bp['boxes'], colors):
    patch.set_facecolor(color)

ax2.set_title('Boxplots comparatifs (outliers visibles)', fontsize=12, fontweight='bold')
ax2.set_ylabel('Rendement mensuel (%)')
ax2.grid(True, alpha=0.3, axis='y')
ax2.axhline(0, color='black', linestyle=':', linewidth=1)

plt.tight_layout()
plt.show()
print("âœ“ Graphiques gÃ©nÃ©rÃ©s (histogrammes + boxplots)")

# ============================================================================ # QUESTION 1.3 â€” VALUE AT RISK (VaR 95%)
# ============================================================================

print("\n" + "="*80)
print("QUESTION 1.3 â€” VALUE AT RISK (VaR 95%)")
print("="*80)

def calculer_var_portefeuille(stats_dict, capital, alpha=0.05):
    """
    Calcule VaR paramÃ©trique mensuelle et annuelle
    Parameters:
    stats_dict : dict
    Statistiques portefeuille (from calculer_stats_portefeuille)
    capital : float
    Capital investi (â‚¬)
    alpha : float
    Niveau risque (0.05 pour VaR 95%)
    Returns:
    dict : VaR calculÃ©es
    """
    # Quantile normal standard pour alpha=5% (queue gauche)
    z_alpha = stats.norm.ppf(alpha) # â‰ˆ -1.645

    # a) VaR mensuelle (%)
    var_mensuelle_pct = stats_dict['moyenne_mensuelle'] + z_alpha * stats_dict['ecart_type_mensuel']

    # b) VaR annuelle (%)
    # MÃ©thode : Utiliser rendement et volatilitÃ© annualisÃ©s
    var_annuelle_pct = stats_dict['rendement_annuel'] + z_alpha * stats_dict['volatilite_annuelle']

    # c) VaR en perte monÃ©taire (â‚¬)
    var_mensuelle_euros = capital * (var_mensuelle_pct / 100)
    var_annuelle_euros = capital * (var_annuelle_pct / 100)

    var_results = {
        'var_mensuelle_pct': var_mensuelle_pct,
        'var_annuelle_pct': var_annuelle_pct,
        'var_mensuelle_euros': var_mensuelle_euros,
        'var_annuelle_euros': var_annuelle_euros
    }

    return var_results

# Calcul VaR pour les deux portefeuilles
var_A = calculer_var_portefeuille(stats_A, capital)
var_B = calculer_var_portefeuille(stats_B, capital)

# Affichage rÃ©sultats
print(f"\nðŸ’° CAPITAL INVESTI : â‚¬{capital:,.0f}")
print(f"ðŸš¨ PERTE MAX TOLÃ‰RÃ‰E CLIENT : â‚¬{perte_max_toleree:,.0f} (-{perte_max_toleree/capital*100:.0f}%)")
print(f"\nðŸ“‰ PORTEFEUILLE A (Conservative)")
print(f"\tâ€¢ VaR 95% mensuelle : {var_A['var_mensuelle_pct']:.2f}% â†’ â‚¬{var_A['var_mensuelle_euros']:,.0f}")
print(f"\tâ€¢ VaR 95% annuelle : {var_A['var_annuelle_pct']:.2f}% â†’ â‚¬{var_A['var_annuelle_euros']:,.0f}")
print(f"\nðŸ“‰ PORTEFEUILLE B (Agressif)")
print(f"\tâ€¢ VaR 95% mensuelle : {var_B['var_mensuelle_pct']:.2f}% â†’ â‚¬{var_B['var_mensuelle_euros']:,.0f}")
print(f"\tâ€¢ VaR 95% annuelle : {var_B['var_annuelle_pct']:.2f}% â†’ â‚¬{var_B['var_annuelle_euros']:,.0f}")

# VÃ©rification contrainte client
print(f"\nâœ… VALIDATION CONTRAINTE CLIENT (VaR annuelle â‰¤ -â‚¬50,000) :")
contrainte_A = abs(var_A['var_annuelle_euros']) <= perte_max_toleree
contrainte_B = abs(var_B['var_annuelle_euros']) <= perte_max_toleree
print(f"\tâ€¢ Portefeuille A : {'âœ“ RESPECTÃ‰E' if contrainte_A else 'âœ— NON RESPECTÃ‰E'} "
      f"({var_A['var_annuelle_euros']:,.0f} â‚¬ vs -{perte_max_toleree:,.0f} â‚¬)")
print(f"\tâ€¢ Portefeuille B : {'âœ“ RESPECTÃ‰E' if contrainte_B else 'âœ— NON RESPECTÃ‰E'} "
      f"({var_B['var_annuelle_euros']:,.0f} â‚¬ vs -{perte_max_toleree:,.0f} â‚¬)")

# d) Test normalitÃ© (Shapiro-Wilk)
print(f"\nðŸ”¬ TEST NORMALITÃ‰ (Shapiro-Wilk, H0: donnÃ©es normales)")

stat_A, p_value_A = stats.shapiro(rendements_A)
stat_B, p_value_B = stats.shapiro(rendements_B)

print(f"\n\tPORTEFEUILLE A :")
print(f"\tâ€¢ Statistique Shapiro : {stat_A:.4f}")
print(f"\tâ€¢ P-value : {p_value_A:.4f}")
if p_value_A > 0.05:
    print(f"\tâœ“ DonnÃ©es compatibles loi normale (p > 0.05)")
else:
    print(f"\tâœ— DonnÃ©es s'Ã©cartent loi normale (p < 0.05) â†’ VaR paramÃ©trique moins fiable")

print(f"\n\tPORTEFEUILLE B :")
print(f"\tâ€¢ Statistique Shapiro : {stat_B:.4f}")
print(f"\tâ€¢ P-value : {p_value_B:.4f}")
if p_value_B > 0.05:
    print(f"\tâœ“ DonnÃ©es compatibles loi normale (p > 0.05)")
else:
    print(f"\tâœ— DonnÃ©es s'Ã©cartent loi normale (p < 0.05) â†’ VaR paramÃ©trique moins fiable")

# ============================================================================ # QUESTION 1.4 â€” RATIO SHARPE ET RECOMMANDATION
# ============================================================================

print("\n" + "="*80)
print("QUESTION 1.4 â€” RATIO SHARPE ET RECOMMANDATION CLIENT")
print("="*80)

# a) Calcul Ratio Sharpe
sharpe_A = (stats_A['rendement_annuel'] - taux_sans_risque) / stats_A['volatilite_annuelle']
sharpe_B = (stats_B['rendement_annuel'] - taux_sans_risque) / stats_B['volatilite_annuelle']
print(f"\nðŸ“Š RATIO SHARPE (Rendement ajustÃ© risque)")

print(f"\tFormule : (Rendement annuel - Taux sans risque) / VolatilitÃ© annuelle")
print(f"\tTaux sans risque (rf) : {taux_sans_risque}%")

print(f"\n\tPORTEFEUILLE A :")
print(f"\tâ€¢ Sharpe = ({stats_A['rendement_annuel']:.2f} - {taux_sans_risque}) / {stats_A['volatilite_annuelle']:.2f}")
print(f"\tâ€¢ Sharpe = {sharpe_A:.3f}")

print(f"\n\tPORTEFEUILLE B :")
print(f"\tâ€¢ Sharpe = ({stats_B['rendement_annuel']:.2f} - {taux_sans_risque}) / {stats_B['volatilite_annuelle']:.2f}")
print(f"\tâ€¢ Sharpe = {sharpe_B:.3f}")

# InterprÃ©tation Sharpe
print(f"\n\tINTERPRÃ‰TATION :")
if sharpe_A > 1:
    print(f"\tâœ“ Portefeuille A : Excellent (Sharpe > 1)")
elif sharpe_A > 0.5:
    print(f"\tâœ“ Portefeuille A : Bon (0.5 < Sharpe < 1)")
else:
    print(f"\tâœ— Portefeuille A : Faible (Sharpe < 0.5)")

if sharpe_B > 1:
    print(f"\tâœ“ Portefeuille B : Excellent (Sharpe > 1)")
elif sharpe_B > 0.5:
    print(f"\tâœ“ Portefeuille B : Bon (0.5 < Sharpe < 1)")
else:
    print(f"\tâœ— Portefeuille B : Faible (Sharpe < 0.5)")

# b) RECOMMANDATION CLIENT
print(f"\n" + "="*80)
print("ðŸŽ¯ RECOMMANDATION CLIENT FINALE")
print("="*80)
print(f"\nðŸ“‹ CRITÃˆRES DÃ‰CISION :")
print(f"\t1. VaR 95% annuelle â‰¤ -â‚¬50,000 (contrainte risque)")
print(f"\t2. Ratio Sharpe maximum (efficience)")
print(f"\t3. NormalitÃ© rendements (fiabilitÃ© VaR)")
print(f"\nðŸ“Š TABLEAU COMPARATIF :")
print(f"\n{'CritÃ¨re':<30} {'Portefeuille A':<20} {'Portefeuille B':<20}")
print(f"{'-'*70}")
print(f"{'Rendement annuel':<30} {stats_A['rendement_annuel']:>8.2f}% {stats_B['rendement_annuel']:>28.2f}%")
print(f"{'VolatilitÃ© annuelle':<30} {stats_A['volatilite_annuelle']:>8.2f}% {stats_B['volatilite_annuelle']:>28.2f}%")
print(f"{'VaR 95% (â‚¬)':<30} {var_A['var_annuelle_euros']:>13,.0f} â‚¬ {var_B['var_annuelle_euros']:>22,.0f} â‚¬")
print(f"{'Contrainte respectÃ©e':<30} {'âœ“ OUI' if contrainte_A else 'âœ— NON':<20} {'âœ“ OUI' if contrainte_B else 'âœ— NON':<20}")
print(f"{'Ratio Sharpe':<30} {sharpe_A:>13.3f} {sharpe_B:>27.3f}")
print(f"{'NormalitÃ© (p-value)':<30} {p_value_A:>13.3f} {p_value_B:>27.3f}")
print(f"\nðŸ’¡ RECOMMANDATION FINALE :")

# Logique dÃ©cision
if not contrainte_A and not contrainte_B:
    print(f"\tâŒ AUCUN PORTEFEUILLE ne respecte contrainte risque client.")
    print(f"\tâ†’ RÃ©duire allocation ou revoir tolÃ©rance perte.")
elif contrainte_A and not contrainte_B:
    print(f"\tâœ… PORTEFEUILLE A (Conservative) RECOMMANDÃ‰")
    print(f"\tâ†’ Seul respecte VaR â‰¤ -â‚¬50,000")
    print(f"\tâ†’ Sharpe {sharpe_A:.2f} correct, volatilitÃ© maÃ®trisÃ©e {stats_A['volatilite_annuelle']:.1f}%")
elif not contrainte_A and contrainte_B:
    print(f"\tâœ… PORTEFEUILLE B (Agressif) RECOMMANDÃ‰")
    print(f"\tâ†’ Seul respecte VaR â‰¤ -â‚¬50,000")
    print(f"\tâ†’ Rendement Ã©levÃ© {stats_B['rendement_annuel']:.1f}% mais volatilitÃ© importante {stats_B['volatilite_annuelle']:.1f}%")
else: # Les deux respectent contrainte
    if sharpe_A > sharpe_B:
        print(f"\tâœ… PORTEFEUILLE A (Conservative) RECOMMANDÃ‰")
        print(f"\tâ†’ Meilleur Sharpe ({sharpe_A:.2f} vs {sharpe_B:.2f})")
        print(f"\tâ†’ Profil risque/rendement optimal selon contrainte client")
    else:
        print(f"\tâœ… PORTEFEUILLE B (Agressif) RECOMMANDÃ‰")
        print(f"\tâ†’ Meilleur Sharpe ({sharpe_B:.2f} vs {sharpe_A:.2f})")
        print(f"\tâ†’ Rendement supÃ©rieur ({stats_B['rendement_annuel']:.1f}% vs {stats_A['rendement_annuel']:.1f}%)")

print(f"\n\tJUSTIFICATION :")
print(f"\tâ€¢ VaR paramÃ©trique fiable si normalitÃ© vÃ©rifiÃ©e (test Shapiro p > 0.05)")
print(f"\tâ€¢ Sharpe mesure efficience : unitÃ©s rendement excÃ©dentaire par unitÃ© risque")
print(f"\tâ€¢ Client conservateur â†’ PrivilÃ©gier A (stabilitÃ©)")
print(f"\tâ€¢ Client tolÃ©rant volatilitÃ© â†’ Envisager B si Sharpe meilleur et VaR OK")
print(f"\nâœ“ FIN PARTIE 1\n")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Configuration
sns.set_style("whitegrid")

print("="*80)
print("TP PARTIE 2 â€” THÃ‰ORÃˆME DE BAYES ET SCORING CRÃ‰DIT")
print("Mise Ã  jour probabilitÃ©s risque avec nouvelles informations")
print("="*80)

# ============================================================================ # DONNÃ‰ES
# ============================================================================

# Taux dÃ©faut base et segmentation
taux_defaut_base = 0.05 # 5%

segments = {
    'Premium': {'proportion': 0.30, 'taux_defaut': 0.015},
    'Standard': {'proportion': 0.50, 'taux_defaut': 0.05},
    'Risque': {'proportion': 0.20, 'taux_defaut': 0.15}
}

# Ã‰vÃ©nements observables (Likelihood)
evenements = {
    'Retard paiement': { 'P(E|Defaut)': 0.80, 'P(E|Non-defaut)': 0.10 },
    'Decouvert >500â‚¬': { 'P(E|Defaut)': 0.65, 'P(E|Non-defaut)': 0.15 },
    'Refus credit ailleurs': { 'P(E|Defaut)': 0.55, 'P(E|Non-defaut)': 0.08 }
}

# ============================================================================ # QUESTION 2.1 â€” CALCUL BAYES MANUEL
# ============================================================================

print("\n" + "="*80)
print("QUESTION 2.1 â€” CALCUL BAYES MANUEL")
print("="*80)
print("\nðŸ“‹ CONTEXTE :")
print("\tClient Segment Standard prÃ©sente un RETARD PAIEMENT")
print(f"\tâ€¢ Prior P(DÃ©faut) = {segments['Standard']['taux_defaut']:.1%}")
print(f"\tâ€¢ P(Retard|DÃ©faut) = {evenements['Retard paiement']['P(E|Defaut)']:.0%}")
print(f"\tâ€¢ P(Retard|Non-dÃ©faut) = {evenements['Retard paiement']['P(E|Non-defaut)']:.0%}")

# a) Calcul Bayes
prior = segments['Standard']['taux_defaut'] # P(DÃ©faut)
likelihood_defaut = evenements['Retard paiement']['P(E|Defaut)'] # P(Retard|DÃ©faut)
likelihood_non_defaut = evenements['Retard paiement']['P(E|Non-defaut)'] # P(Retard|Non-dÃ©faut)

# P(Retard) = P(Retard|DÃ©faut)Ã—P(DÃ©faut) + P(Retard|Non-dÃ©faut)Ã—P(Non-dÃ©faut)
p_retard = likelihood_defaut * prior + likelihood_non_defaut * (1 - prior)

# P(DÃ©faut|Retard) = P(Retard|DÃ©faut) Ã— P(DÃ©faut) / P(Retard)
posterior = (likelihood_defaut * prior) / p_retard
print(f"\nðŸ§® CALCUL DÃ‰TAILLÃ‰ BAYES :")
print(f"\n\tÃ‰tape 1 : Calcul P(Retard) via loi probabilitÃ©s totales")
print(f"\tP(Retard) = P(Retard|DÃ©faut)Ã—P(DÃ©faut) + P(Retard|Non-dÃ©faut)Ã—P(Non-dÃ©faut)")
print(f"\tP(Retard) = {likelihood_defaut:.2f} Ã— {prior:.2f} + {likelihood_non_defaut:.2f} Ã— {1-prior:.2f}")
print(f"\tP(Retard) = {likelihood_defaut * prior:.4f} + {likelihood_non_defaut * (1-prior):.4f}")
print(f"\tP(Retard) = {p_retard:.4f} = {p_retard:.2%}")

print(f"\n\tÃ‰tape 2 : ThÃ©orÃ¨me de Bayes")
print(f"\tP(DÃ©faut|Retard) = P(Retard|DÃ©faut) Ã— P(DÃ©faut) / P(Retard)")
print(f"\tP(DÃ©faut|Retard) = {likelihood_defaut:.2f} Ã— {prior:.2f} / {p_retard:.4f}")
print(f"\tP(DÃ©faut|Retard) = {likelihood_defaut * prior:.4f} / {p_retard:.4f}")
print(f"\tP(DÃ©faut|Retard) = {posterior:.4f} = {posterior:.2%}")

# b) InterprÃ©tation facteur multiplicatif
facteur_multiplication = posterior / prior
print(f"\nðŸ“Š INTERPRÃ‰TATION :")
print(f"\tâ€¢ Prior (avant retard) : {prior:.1%}")
print(f"\tâ€¢ Posterior (aprÃ¨s retard) : {posterior:.1%}")
print(f"\tâ€¢ Augmentation risque : {(posterior - prior)*100:.1f} points")
print(f"\tâ€¢ Facteur multiplication : Ã—{facteur_multiplication:.2f}")
print(f"\tâ†’ Retard paiement MULTIPLIE risque dÃ©faut par {facteur_multiplication:.1f} !")

# c) DÃ©cision mÃ©tier
print(f"\nðŸ’¡ DÃ‰CISION MÃ‰TIER RECOMMANDÃ‰E :")
if posterior < 0.15:
    decision = "SURVEILLANCE STANDARD"
    action = "Monitoring mensuel, pas de restriction immÃ©diate"
elif posterior < 0.30:
    decision = "SURVEILLANCE RENFORCÃ‰E"
    action = "Monitoring hebdomadaire, limite dÃ©couvert rÃ©duite -30%"
else:
    decision = "RESTRICTION CRÃ‰DIT"
    action = "Blocage nouveaux crÃ©dits, rÃ©duction plafond carte -50%"

print(f"\tâœ“ DÃ‰CISION : {decision}")
print(f"\tâœ“ ACTION : {action}")
print(f"\tâœ“ JUSTIFICATION : Posterior {posterior:.1%} franchit seuil alerte 15%")

# ============================================================================ # QUESTION 2.2 â€” MISE Ã€ JOUR SÃ‰QUENTIELLE
# ============================================================================

print("\n" + "="*80)
print("QUESTION 2.2 â€” MISE Ã€ JOUR SÃ‰QUENTIELLE")
print("="*80)
print("\nðŸ“‹ CONTEXTE :")
print("\t2 semaines aprÃ¨s, le mÃªme client prÃ©sente DÃ‰COUVERT >500â‚¬")
print("\tâ†’ Utilisation posterior Q2.1 comme nouveau prior")

# a) Nouveau prior = posterior Q2.1
prior_2 = posterior

# b) Calcul Bayes dÃ©couvert
likelihood_defaut_2 = evenements['Decouvert >500â‚¬']['P(E|Defaut)']
likelihood_non_defaut_2 = evenements['Decouvert >500â‚¬']['P(E|Non-defaut)']

p_decouvert = likelihood_defaut_2 * prior_2 + likelihood_non_defaut_2 * (1 - prior_2)
posterior_2 = (likelihood_defaut_2 * prior_2) / p_decouvert
print(f"\nðŸ§® CALCUL BAYES (Ã‰VÃ‰NEMENT 2 : DÃ‰COUVERT) :")


print(f"\tâ€¢ Nouveau prior P(DÃ©faut) = {prior_2:.4f} (= posterior Q2.1)")
print(f"\tâ€¢ P(DÃ©couvert|DÃ©faut) = {likelihood_defaut_2:.0%}")
print(f"\tâ€¢ P(DÃ©couvert|Non-dÃ©faut) = {likelihood_non_defaut_2:.0%}")

print(f"\n\t\tP(DÃ©couvert) = {likelihood_defaut_2:.2f} Ã— {prior_2:.4f} + {likelihood_non_defaut_2:.2f} Ã— {1-prior_2:.4f}")
print(f"\tP(DÃ©couvert) = {p_decouvert:.4f}")

print(f"\n\t\tP(DÃ©faut|Retard ET DÃ©couvert) = {likelihood_defaut_2:.2f} Ã— {prior_2:.4f} / {p_decouvert:.4f}")
print(f"\tP(DÃ©faut|Retard ET DÃ©couvert) = {posterior_2:.4f} = {posterior_2:.2%}")
print(f"\nðŸ“Š Ã‰VOLUTION PROBABILITÃ‰ DÃ‰FAUT :")
print(f"\tÃ‰tape 0 (Prior initial) : {prior:.1%}")
print(f"\tÃ‰tape 1 (AprÃ¨s Retard) : {posterior:.1%} (+{(posterior-prior)*100:.1f} pts)")
print(f"\tÃ‰tape 2 (AprÃ¨s DÃ©couvert) : {posterior_2:.1%} (+{(posterior_2-posterior)*100:.1f} pts)")
print(f"\tâ†’ TOTAL : Ã—{posterior_2/prior:.2f} augmentation risque depuis prior initial")

# c) Graphique Ã©volution
fig, ax = plt.subplots(figsize=(10, 6))

etapes = ['0\nPrior initial\n(Segment Standard)', '1\nAprÃ¨s\nRetard paiement', '2\nAprÃ¨s\nDÃ©couvert >500â‚¬']
probas = [prior * 100, posterior * 100, posterior_2 * 100]

ax.plot(range(3), probas, marker='o', markersize=12, linewidth=3, color='darkred', label='ProbabilitÃ© dÃ©faut')

# Points
for i, (etape, proba) in enumerate(zip(etapes, probas)):
    ax.annotate(f'{proba:.2f}%', xy=(i, proba), xytext=(0, 10),
    textcoords='offset points', ha='center', fontsize=11, fontweight='bold', bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))

# Seuils dÃ©cision
ax.axhline(15, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='Seuil surveillance renforcÃ©e (15%)')
ax.axhline(30, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Seuil restriction crÃ©dit (30%)')

ax.set_xticks(range(3))
ax.set_xticklabels(etapes)
ax.set_ylabel('ProbabilitÃ© dÃ©faut (%)', fontsize=12, fontweight='bold')
ax.set_title('Mise Ã  jour sÃ©quentielle risque crÃ©dit (ThÃ©orÃ¨me Bayes)',
fontsize=13, fontweight='bold')
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
print("\nâœ“ Graphique Ã©volution probabilitÃ© gÃ©nÃ©rÃ©")

# ============================================================================ # QUESTION 2.3 â€” FONCTION GÃ‰NÃ‰RIQUE BAYES
# ============================================================================

print("\n" + "="*80)
print("QUESTION 2.3 â€” FONCTION GÃ‰NÃ‰RIQUE BAYES")
print("="*80)

def bayes_update(prior, likelihood_pos, likelihood_neg):
    """
    Calcule probabilitÃ© a posteriori via thÃ©orÃ¨me de Bayes

    Formule : P(A|B) = P(B|A) Ã— P(A) / P(B)
    Avec P(B) = P(B|A)Ã—P(A) + P(B|Â¬A)Ã—P(Â¬A)

    Parameters:

    prior : float
    ProbabilitÃ© a priori P(A) âˆˆ [0, 1]
    Exemple : 0.05 pour taux dÃ©faut 5%

    likelihood_pos : float
    Vraisemblance P(Evidence|Positive) âˆˆ [0, 1]
    Exemple : 0.80 pour P(Retard|DÃ©faut)

    likelihood_neg : float
    Vraisemblance P(Evidence|Negative) âˆˆ [0, 1]
    Exemple : 0.10 pour P(Retard|Non-dÃ©faut)

    Returns:

    posterior : float
    ProbabilitÃ© a posteriori P(A|B) âˆˆ [0, 1]
    Raises:
    ValueError : Si paramÃ¨tres hors [0, 1]
    Examples:
    >>> # Client dÃ©faut 5%, observe retard (80% si dÃ©faut, 10% si sain)
    >>> posterior = bayes_update(prior=0.05, likelihood_pos=0.80, likelihood_neg=0.10)
    >>> print(f"P(DÃ©faut|Retard) = {posterior:.2%}")
    P(DÃ©faut|Retard) = 29.63%

    >>> # Mise Ã  jour sÃ©quentielle : posterior devient nouveau prior
    >>> posterior_2 = bayes_update(prior=posterior, likelihood_pos=0.65, likelihood_neg=0.15)
    >>> print(f"P(DÃ©faut|Retard ET DÃ©couvert) = {posterior_2:.2%}")
    P(DÃ©faut|Retard ET DÃ©couvert) = 55.88%
    """
    # Validation inputs
    if not (0 <= prior <= 1):
        raise ValueError(f"prior doit Ãªtre dans [0,1], reÃ§u {prior}")
    if not (0 <= likelihood_pos <= 1):
        raise ValueError(f"likelihood_pos doit Ãªtre dans [0,1], reÃ§u {likelihood_pos}")
    if not (0 <= likelihood_neg <= 1):
        raise ValueError(f"likelihood_neg doit Ãªtre dans [0,1], reÃ§u {likelihood_neg}")

    # Calcul P(Evidence) via loi probabilitÃ©s totales
    p_evidence = likelihood_pos * prior + likelihood_neg * (1 - prior)

    # Protection division par zÃ©ro
    if p_evidence == 0:
        return 0.0

    # ThÃ©orÃ¨me de Bayes
    posterior = (likelihood_pos * prior) / p_evidence
    return posterior
print("\nâœ“ Fonction bayes_update() crÃ©Ã©e avec docstring complÃ¨te")

# Test fonction sur Client Segment Risque (prior 15%)
print(f"\nðŸ§ª TEST FONCTION â€” Client Segment RISQUE (prior dÃ©faut {segments['Risque']['taux_defaut']:.0%})")

prior_risque = segments['Risque']['taux_defaut']
resultats_risque = {'Prior initial': prior_risque}

# Ã‰vÃ©nement 1 : Retard
post_1 = bayes_update(
    prior=prior_risque,
    likelihood_pos=evenements['Retard paiement']['P(E|Defaut)'],
    likelihood_neg=evenements['Retard paiement']['P(E|Non-defaut)']
)
resultats_risque['AprÃ¨s Retard'] = post_1

# Ã‰vÃ©nement 2 : DÃ©couvert
post_2 = bayes_update(
    prior=post_1,
    likelihood_pos=evenements['Decouvert >500â‚¬']['P(E|Defaut)'],
    likelihood_neg=evenements['Decouvert >500â‚¬']['P(E|Non-defaut)']
)
resultats_risque['AprÃ¨s DÃ©couvert'] = post_2

# Ã‰vÃ©nement 3 : Refus crÃ©dit
post_3 = bayes_update(
    prior=post_2,
    likelihood_pos=evenements['Refus credit ailleurs']['P(E|Defaut)'],
    likelihood_neg=evenements['Refus credit ailleurs']['P(E|Non-defaut)']
)
resultats_risque['AprÃ¨s Refus crÃ©dit'] = post_3
print(f"\nðŸ“Š RÃ‰SULTATS TEST (Client Segment Risque) :")
for etape, proba in resultats_risque.items():
    print(f"\t{etape:<25} : P(DÃ©faut) = {proba:.4f} ({proba:.2%})")
print(f"\nðŸ’¡ INTERPRÃ‰TATION :")
print(f"\tâ€¢ Risque initial : {prior_risque:.0%} (segment risque)")
print(f"\tâ€¢ AprÃ¨s 3 Ã©vÃ©nements nÃ©gatifs : {post_3:.0%}")
print(f"\tâ€¢ Multiplication risque : Ã—{post_3/prior_risque:.1f}")
print(f"\tâ†’ Client trÃ¨s haut risque, recommandation : REJET crÃ©dit ou garanties renforcÃ©es")

# ============================================================================ # QUESTION 2.4 â€” MATRICE CONFUSION ET LIEN BAYES
# ============================================================================

print("\n" + "="*80)
print("QUESTION 2.4 â€” MATRICE CONFUSION ET LIEN BAYES")
print("="*80)
print("\nðŸ“‹ DONNÃ‰ES MATRICE CONFUSION (10,000 clients testÃ©s) :")

n_total = 10000
n_defauts_reels = 500 # 5% taux dÃ©faut
n_non_defauts_reels = n_total - n_defauts_reels

tp = 400 # Vrais positifs (dÃ©fauts dÃ©tectÃ©s par retard)
fp = 950 # Faux positifs (non-dÃ©fauts avec retard)
fn = n_defauts_reels - tp # Faux nÃ©gatifs
tn = n_non_defauts_reels - fp # Vrais nÃ©gatifs

print(f"\tâ€¢ Total clients : {n_total:,}")
print(f"\tâ€¢ DÃ©fauts rÃ©els : {n_defauts_reels} ({n_defauts_reels/n_total:.0%})")
print(f"\tâ€¢ Vrais positifs (TP) : {tp}")
print(f"\tâ€¢ Faux positifs (FP) : {fp}")
print(f"\tâ€¢ Faux nÃ©gatifs (FN) : {fn}")
print(f"\tâ€¢ Vrais nÃ©gatifs (TN) : {tn}")

print(f"\n\tMATRICE CONFUSION")
print(f"\t\tRÃ‰ALITÃ‰")
print(f"\tNon-dÃ©faut DÃ©faut")
print(f"PRÃ‰D Retard\t\t{fp:4d}\t\t{tp:3d}")
print(f"\tPas\t{tn:4d}\t{fn:3d}")

# a) Calcul Precision
precision = tp / (tp + fp)
print(f"\nðŸ§® CALCUL PRECISION :")
print(f"\tPrecision = TP / (TP + FP)")
print(f"\tPrecision = {tp} / ({tp} + {fp})")
print(f"\tPrecision = {tp} / {tp + fp}")
print(f"\tPrecision = {precision:.4f} = {precision:.2%}")

# b) Comparaison avec Bayes Q2.1
print(f"\nðŸ”— COMPARAISON AVEC BAYES Q2.1 :")
print(f"\tâ€¢ P(DÃ©faut|Retard) calculÃ© Bayes : {posterior:.4f} ({posterior:.2%})")
print(f"\tâ€¢ Precision matrice confusion : {precision:.4f} ({precision:.2%})")
print(f"\tâ€¢ DiffÃ©rence : {abs(posterior - precision):.4f} ({abs(posterior - precision)*100:.2f} pts)")

if abs(posterior - precision) < 0.01:
    print(f"\tâœ“ COHÃ‰RENCE PARFAITE (< 1 pt diffÃ©rence)")
else:
    print(f"\tâš  Petite diffÃ©rence due arrondis ou donnÃ©es simulÃ©es")

# c) Explication lien Bayes / Precision
print(f"\nðŸ’¡ EXPLICATION LIEN BAYES â†” PRECISION :")
print(f"""
THÃ‰ORÃˆME DE BAYES :
P(DÃ©faut|Retard) = "Parmi clients avec RETARD, quelle % sont DÃ‰FAUTS rÃ©els ?"

PRECISION (Matrice confusion) :
Precision = TP / (TP + FP) = "Parmi prÃ©dictions POSITIVES, quelle % correctes ?"

â†’ Ã‰QUIVALENCE MATHÃ‰MATIQUE :
PrÃ©cision mesure P(Classe vraie | PrÃ©diction positive)
= P(DÃ©faut rÃ©el | Retard dÃ©tectÃ©)
= Calcul bayÃ©sien de posterior !

â†’ En ML, optimiser Precision = maximiser probabilitÃ©s a posteriori bayÃ©siennes
â†’ Naive Bayes Classifier utilise explicitement P(Classe|Features) via Bayes """)
print(f"\nâœ“ FIN PARTIE 2\n")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (
    confusion_matrix, classification_report, roc_curve, roc_auc_score, auc,
    precision_score, recall_score, f1_score, accuracy_score, precision_recall_curve
)

# Configuration
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 5)

print("="*80)
print("TP PARTIE 3 â€” K-NEAREST NEIGHBORS (KNN) ET Ã‰VALUATION MODÃˆLE")
print("Classification crÃ©dit et optimisation hyperparamÃ¨tres")
print("="*80)

# ============================================================================

"""# Task
The user wants to perform a credit risk classification task using a KNN model. The first step involves generating a synthetic dataset for credit scoring, exploring its characteristics through descriptive statistics and correlation analysis, and visualizing the data distribution and relationships using a correlation heatmap and boxplots.

## Data Exploration and Visualization (Q3.1)

### Subtask:
Generate a synthetic dataset for credit scoring, calculate descriptive statistics, analyze correlations, and create two visualizations (e.g., heatmap and boxplots) to understand the data.

**Reasoning**:
The first step is to generate the synthetic credit scoring dataset as specified in the instructions. This involves creating columns with specific distributions and ensuring the target variable `defaut_paiement` is correlated with other features.
"""

np.random.seed(42)

n_samples = 1500 # Using 1500 samples for better representation

# 1. revenu_mensuel (monthly income): Normally distributed around 3000-5000 with some spread.
revenu_mensuel = np.random.normal(loc=4000, scale=1000, size=n_samples)
revenu_mensuel = np.maximum(500, revenu_mensuel) # Ensure positive income

# 2. anciennete_emploi (employment seniority in years): Random integers between 0 and 20.
anciennete_emploi = np.random.randint(0, 21, size=n_samples)

# 3. taux_endettement (debt-to-income ratio): Random floats between 0 and 0.8.
taux_endettement = np.random.uniform(0, 0.8, size=n_samples)

# 4. nbr_credits (number of active credits): Random integers between 0 and 5.
nbr_credits = np.random.randint(0, 6, size=n_samples)

# 5. score_credit_ficp (credit score, e.g., FICO-like): Normally distributed around 600-750 with some spread.
score_credit_ficp = np.random.normal(loc=675, scale=75, size=n_samples)
score_credit_ficp = np.clip(score_credit_ficp, 300, 850) # Clip to typical credit score range

# 6. defaut_paiement (payment default): Target variable
# Make default correlated with other features (lower income, higher debt, lower credit score)
# Aim for a reasonable proportion of defaults (e.g., 15-25%)

default_probability = (
    0.00005 * (8000 - revenu_mensuel) +  # Lower income -> higher default prob
    0.4 * taux_endettement +              # Higher debt -> higher default prob
    0.003 * (300 - score_credit_ficp) +   # Lower credit score -> higher default prob
    0.05 * nbr_credits                    # More credits -> higher default prob
)

default_probability = np.clip(default_probability, 0.05, 0.7) # Clip probabilities to a reasonable range

defaut_paiement = (np.random.rand(n_samples) < default_probability).astype(int)

# Create DataFrame
df_credit = pd.DataFrame({
    'revenu_mensuel': revenu_mensuel,
    'anciennete_emploi': anciennete_emploi,
    'taux_endettement': taux_endettement,
    'nbr_credits': nbr_credits,
    'score_credit_ficp': score_credit_ficp,
    'defaut_paiement': defaut_paiement
})

# Display the first few rows of df_credit
print("First 5 rows of the generated df_credit:")
print(df_credit.head())

# Check the proportion of defaults
default_proportion = df_credit['defaut_paiement'].mean()
print(f"\nProportion of defaults: {default_proportion:.2%}")

"""**Reasoning**:
Generate and display descriptive statistics for all numerical columns in `df_credit` using the `.describe()` method to understand the data's central tendency, dispersion, and shape.


"""

print("\nDescriptive statistics for df_credit:")
print(df_credit.describe())

"""**Reasoning**:
Calculate the correlation matrix for `df_credit` to understand the linear relationships between the generated features, as specified in instruction 4.


"""

print("\nCorrelation matrix for df_credit:")
correlation_matrix = df_credit.corr()
print(correlation_matrix)

"""**Reasoning**:
Now that the correlation matrix has been calculated, I will create a heatmap to visually represent the correlations between features in `df_credit`, as required by instruction 5. This will provide a clear overview of relationships within the dataset.


"""

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)
plt.title('Correlation Matrix of Credit Scoring Features', fontsize=16)
plt.show()
print("âœ“ Correlation heatmap generated.")

"""**Reasoning**:
To complete the visualization requirements, I will generate box plots for each numerical feature in `df_credit`, comparing their distributions against the `defaut_paiement` target variable. This will fulfill instruction 6 of the subtask.


"""

numerical_features = ['revenu_mensuel', 'anciennete_emploi', 'taux_endettement', 'nbr_credits', 'score_credit_ficp']

plt.figure(figsize=(15, 12))
for i, feature in enumerate(numerical_features):
    plt.subplot(3, 2, i + 1) # Create 3 rows, 2 columns of subplots
    sns.boxplot(x='defaut_paiement', y=feature, data=df_credit)
    plt.title(f'Distribution of {feature} by Default Status')
    plt.xlabel('Default (0: No, 1: Yes)')
    plt.ylabel(feature)

plt.tight_layout()
plt.show()
print("âœ“ Box plots for numerical features by default status generated.")

"""## Data Preprocessing (Q3.2)

### Subtask:
Split the dataset into training and testing sets using a stratified 70/30 split, apply StandardScaler for feature scaling, and perform checks to ensure proper normalization.

**Reasoning**:
The first step in data preprocessing is to separate the features (X) from the target variable (y). This sets up the data correctly for splitting into training and testing sets.
"""

X = df_credit.drop('defaut_paiement', axis=1)
y = df_credit['defaut_paiement']

print(f"Shape of X (features): {X.shape}")
print(f"Shape of y (target): {y.shape}")

"""**Reasoning**:
Now that features and target are separated, the next step is to split the dataset into training and testing sets using a stratified 70/30 split to ensure both sets have a similar proportion of the target variable, which is crucial for classification tasks.


"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_test: {y_test.shape}")

# Verify stratification
print(f"\nProportion of defaults in y_train: {y_train.mean():.2%}")
print(f"Proportion of defaults in y_test: {y_test.mean():.2%}")

"""**Reasoning**:
Now, as per instruction 4, I will initialize a `StandardScaler` and apply it to `X_train` and `X_test` to scale the features. This is a crucial step for many machine learning algorithms, including KNN, as it standardizes the input features.


"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"X_train_scaled shape: {X_train_scaled.shape}")
print(f"X_test_scaled shape: {X_test_scaled.shape}")

"""**Reasoning**:
To ensure proper normalization and verify that the scaling was successful, I will now print the mean and standard deviation of `X_train_scaled` and `X_test_scaled`. This will confirm that the features have been transformed to have a mean close to 0 and a standard deviation close to 1, as per instruction 5.


"""

print("\nMean of X_train_scaled:\n", X_train_scaled.mean(axis=0))
print("\nStandard deviation of X_train_scaled:\n", X_train_scaled.std(axis=0))

print("\nMean of X_test_scaled:\n", X_test_scaled.mean(axis=0))
print("\nStandard deviation of X_test_scaled:\n", X_test_scaled.std(axis=0))

print("âœ“ Scaling verified: mean close to 0, std dev close to 1.")